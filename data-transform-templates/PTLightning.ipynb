{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d6f536",
   "metadata": {},
   "source": [
    "## Vùng để code Augment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52523011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from torch.utils.data import Dataset,DataLoader\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pytorch_lightning.loggers import TensorBoardLogger\n",
      "from pytorch_lightning.callbacks import ModelCheckpoint\n",
      "from torchmetrics.functional.classification import accuracy\n",
      "from tokenizers import Tokenizer\n",
      "from tokenizers.models import WordPiece\n",
      "from tokenizers.trainers import WordPieceTrainer\n"
     ]
    }
   ],
   "source": [
    "print(\"from torch.utils.data import Dataset,DataLoader||from sklearn.model_selection import train_test_split||from pytorch_lightning.loggers import TensorBoardLogger||from pytorch_lightning.callbacks import ModelCheckpoint||from torchmetrics.functional.classification import accuracy||from tokenizers import Tokenizer||from tokenizers.models import WordPiece||from tokenizers.trainers import WordPieceTrainer\".replace(\"||\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"from torch.utils.data import Dataset,DataLoader||from sklearn.model_selection import train_test_split||from pytorch_lightning.loggers import TensorBoardLogger||from pytorch_lightning.callbacks import ModelCheckpoint||from torchmetrics.functional.classification import accuracy||from tokenizers import Tokenizer||from tokenizers.models import WordPiece||from tokenizers.trainers import WordPieceTrainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.vi import Vietnamese\n",
    "import string\n",
    "\n",
    "nlp = Vietnamese()\n",
    "\n",
    "\n",
    "def text_normalize(\n",
    "    text,\n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    remove_mentions=True,\n",
    "    remove_hashtags=True,\n",
    "    remove_numbers=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Normalize and clean Vietnamese text with additional regex steps:\n",
    "    - remove/normalize URLs, emails, mentions, hashtags, phone numbers, currency symbols\n",
    "    - remove non-printable characters, reduce repeated punctuation and elongated letters\n",
    "    - optional removal of digits\n",
    "    Returns cleaned string (tokens joined by space) after spaCy tokenization and stopword filtering.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = str(text)\n",
    "        # basic normalization\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # remove urls\n",
    "        if remove_urls:\n",
    "            text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text)\n",
    "        # remove emails\n",
    "        if remove_emails:\n",
    "            text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "        # remove html tags\n",
    "        text = re.sub(r\"<.*?>\", \" \", text)\n",
    "        # mentions (@username)\n",
    "        if remove_mentions:\n",
    "            text = re.sub(r\"@\\w+\", \" \", text)\n",
    "        # hashtags: keep the word but drop the #\n",
    "        if remove_hashtags:\n",
    "            text = re.sub(r\"#(\\w+)\", r\"\\0\", text)\n",
    "        # remove phone numbers (simple patterns)\n",
    "        text = re.sub(r\"\\b-1\\d{8,}\\b|\\b\\d{9,}\\b\", \" \", text)\n",
    "        # replace common currency symbols with space\n",
    "        text = re.sub(r\"[\\$€£¥₫]\", \" \", text)\n",
    "\n",
    "        # remove punctuation (translate to space to avoid joining words)\n",
    "        text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \" \", text)\n",
    "\n",
    "        # remove non-printable characters\n",
    "        text = \"\".join(ch for ch in text if ch.isprintable())\n",
    "\n",
    "        # collapse repeated punctuation (e.g., '!!!' -> '!')\n",
    "        text = re.sub(r\"([!?.]){1,}\", r\"\\1\", text)\n",
    "        # reduce elongated characters (>1 repeats -> 2 repeats) e.g. heyyyy -> heyy\n",
    "        text = re.sub(r\"(.)\\0{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "        # optionally remove digits entirely\n",
    "        if remove_numbers:\n",
    "            text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "        # normalize whitespace\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # tokenize with spaCy (Vietnamese) and filter stopwords/punct/space\n",
    "        doc = nlp(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Keep tokens that are not stopwords/punctuation/space\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                # filter out tokens that contain digits (optional already handled above)\n",
    "                if not any(char.isdigit() for char in token.text):\n",
    "                    cleaned_tokens.append(token.lower_)\n",
    "\n",
    "        return \" \".join(cleaned_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error normalizing text:\", e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel, BPE, WordPiece, Unigram\n",
    "from tokenizers.trainers import (\n",
    "    WordLevelTrainer,\n",
    "    BpeTrainer,\n",
    "    WordPieceTrainer,\n",
    "    UnigramTrainer,\n",
    ")\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd\n",
    "\n",
    "encodings = [\"utf-8\", \"utf-8-sig\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
    "\n",
    "for enc in encodings:\n",
    "    try:\n",
    "        df = pd.read_csv(\"./Attachments/train.csv\", encoding=enc)\n",
    "        print(f\"Read finish with: {enc}\")\n",
    "        print(df.head())\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Cant read with: {enc}\")\n",
    "\n",
    "corpus = df[\"question_text\"].apply(lambda x: str(x)).tolist()  # type: ignore\n",
    "\n",
    "data_size = len(corpus)\n",
    "\n",
    "\n",
    "def create_tokenizer(vocab_size: int = 31000, sequence_length: int = 64):\n",
    "    # tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))  # type: ignore\n",
    "    tokenizer = Tokenizer(Unigram()) # type: ignore\n",
    "    tokenizer.pre_tokenizer = Whitespace()  # type: ignore\n",
    "    tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\", length=sequence_length)\n",
    "    tokenizer.enable_truncation(max_length=sequence_length)\n",
    "    trainer = UnigramTrainer(\n",
    "        vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \"[MASK]\"],\n",
    "        unk_token=\"[UNK]\",\n",
    "    )\n",
    "    # trainer = WordPieceTrainer(\n",
    "    #     vocab_size=vocab_size,\n",
    "    #     special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \"[MASK]\"],\n",
    "    #     unk_token=\"[UNK]\",\n",
    "    # )\n",
    "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "    tokenizer.save(\"./Attachments/tokenizer-uni.json\")\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f3047",
   "metadata": {},
   "source": [
    "## Biến đổi dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "encodings = [\"utf-8\", \"utf-8-sig\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
    "\n",
    "for enc in encodings:\n",
    "    try:\n",
    "        df = pd.read_csv(\"./Attachments/train.csv\", encoding=enc).dropna()\n",
    "        # df = pd.read_csv(\"../Attachments/NER_dataset_cleaned.csv\", encoding=enc)\n",
    "        print(f\"Read finish with: {enc}\")\n",
    "        print(df.head())\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Cant read with: {enc}\")\n",
    "\n",
    "\n",
    "for enc in encodings:\n",
    "    try:\n",
    "        df_test = pd.read_csv(\"./Attachments/test.csv\", encoding=enc).dropna()\n",
    "        # df = pd.read_csv(\"../Attachments/NER_dataset_cleaned.csv\", encoding=enc)\n",
    "        print(f\"Read finish with: {enc}\")\n",
    "        print(df_test.head())\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Cant read with: {enc}\")\n",
    "\n",
    "df_0 = df[df[\"target\"] == 0].iloc[:80000]\n",
    "df_1 = df[df[\"target\"] == 1]\n",
    "df = pd.concat([df_0, df_1])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "class CustomCollator(Dataset):\n",
    "    def __init__(self, questions, target, tokenizer, is_training= True):\n",
    "        self.questions = questions\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # áp dụng biến đổi encoding và normalization vào dữ liệu\n",
    "        question = text_normalize(self.questions[idx])\n",
    "        encoding = self.tokenizer.encode(question).ids\n",
    "        \"\"\"\n",
    "        NOTE: tokenizer dùng cho huggingface AutoTokenizer.from_pretrained thì sẽ trả về một dict gồm các key như input_ids, attention_mask, token_type_ids,...\n",
    "        nên khi dùng huggingface thì phần này sẽ là:\n",
    "        encoding = self.tokenizer(question, padding='max_length', truncation=True, max_length=sequence_length, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),  # remove batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_training:\n",
    "            target_id = [0, 0]\n",
    "            target_id[int(self.target[idx])] = 1\n",
    "            return (\n",
    "                torch.tensor(encoding, dtype=torch.long),\n",
    "                torch.tensor(target_id, dtype=torch.float),\n",
    "            )\n",
    "        else:\n",
    "            return torch.tensor(encoding, dtype=torch.long)\n",
    "\n",
    "\n",
    "tokenizer = create_tokenizer()\n",
    "# tokenizer = Tokenizer.from_file(\"./Attachments/tokenizer-wp.json\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[\"question_text\"].apply(lambda x: str(x)).tolist(),  # type: ignore\n",
    "    df[\"target\"].apply(lambda x: int(x)).tolist(),  # type: ignore\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_test = df_test[\"question_text\"].apply(lambda x: str(x)).tolist()  # type: ignore\n",
    "\n",
    "training_dataset = CustomCollator(X_train, y_train, tokenizer)  # type: ignore\n",
    "validation_dataset = CustomCollator(X_val, y_val, tokenizer)  # type: ignore\n",
    "testing_dataset = CustomCollator(X_test, y_val, tokenizer, is_training=False)  # type: ignore\n",
    "\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_dataset, batch_size=128, shuffle=True, drop_last=True\n",
    ")  # type: ignore\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, batch_size=128, shuffle=False, drop_last=True\n",
    ")  # type: ignore\n",
    "\n",
    "testing_loader = DataLoader(\n",
    "    testing_dataset, batch_size=128, shuffle=False, drop_last=False\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a9097",
   "metadata": {},
   "source": [
    "## Áp mô hình \n",
    " - pytorch lightning thì sẽ có những hàm default thì chỉ cần gọi ra là xong \n",
    " - metric nó có sẵn rồi với thư viện torchmetrics.functional.<bài toán> \n",
    " - chủ yếu xoay quanh những hàm hữu ích sau: \n",
    "    1. forward\n",
    "    2. training_step\n",
    "    3. vaidation_step\n",
    "    4. test_step (bước này phải gọi trainer.test)\n",
    "    5. configure_optimizers (này thì fix cứng)\n",
    " - sử dụng self.log để hiển thị log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MyModel(L.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Model components\n",
    "        \"\"\"\n",
    "        self.trans = nn.Transformer(\n",
    "            d_model=input_dim,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            num_encoder_layers=4,\n",
    "            num_decoder_layers=2,\n",
    "            nhead=8,\n",
    "            # dim_feedforward=64*256,\n",
    "            # norm_first=True,\n",
    "        )  # 32x64x64\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set loss với acc \n",
    "        self.criterion = torch.nn.CrossEntropyLoss() \n",
    "        self.accuracy = accuracy\n",
    "\n",
    "        # Define your model architecture here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass logits computation\n",
    "        \"\"\"\n",
    "        transformer_out = self.trans.encoder(x)  # 32x64x64\n",
    "        transformer_out = self.dense(transformer_out[:, 0, :])  # get [CLS] token  32x2\n",
    "        return transformer_out\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def test_val_step(self, batch, batch_idx):\n",
    "        # Shared step for validation and testing\n",
    "        \"\"\"\n",
    "        x, y = batch # NOTE: Phụ thuộc vào return từ CustomCollator \n",
    "        logits: torch.Tensor = self(x)  # 32x2\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy( \n",
    "            logits.argmax(dim=-1),\n",
    "            y.argmax(dim=-1),\n",
    "            num_classes=self.output_dim,\n",
    "            task=\"multiclass\",\n",
    "        ) # type: ignore\n",
    "        return loss, acc\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        loss, acc = self.test_val_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return { # training step phải có trả về {\"loss\": loss} \n",
    "            \"loss\": loss,\n",
    "            \"accuracy\": acc,\n",
    "        }\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # gọi lại hàm trên thôi \n",
    "        \"\"\"\n",
    "        loss, acc = self.test_val_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        all_logits = self.validation_results[\"logits\"]\n",
    "        all_labels = self.validation_results[\"labels\"]\n",
    "\n",
    "        ## NOTE: Plot confusion matrix to TensorBoard\n",
    "        cm = confusion_matrix(\n",
    "            all_labels,\n",
    "            all_logits,\n",
    "            labels=[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        )\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm\n",
    "        )\n",
    "        disp.plot(ax=ax)\n",
    "\n",
    "        f1_micro = f1_score(\n",
    "            self.validation_results[\"logits\"],\n",
    "            self.validation_results[\"labels\"],\n",
    "            average=\"micro\",\n",
    "        )\n",
    "\n",
    "        self.logger.experiment.add_figure(\n",
    "            \"validation/confusion_matrix\", fig, self.current_epoch\n",
    "        )\n",
    "        self.log(\"validation/f1_micro\", f1_micro, prog_bar=True)\n",
    "\n",
    "\n",
    "        self.validation_results[\"logits\"].clear()\n",
    "        self.validation_results[\"labels\"].clear()\n",
    "        self.validation_results[\"texts\"].clear()\n",
    "        print(f\"F1 Micro: {f1_micro}\")\n",
    "\n",
    "    def configure_optimizers(self):  # type: ignore\n",
    "        # Cái này thì sét cứng chỉ cần thay đổi Lr thôi \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", patience=3\n",
    "            ),\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model_name = \"Text_Classifier_Model\"\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=model_name)\n",
    "    \n",
    "    # Save checkpoint điểm có val_acc tốt nhất \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_acc\",\n",
    "        dirpath=f\"checkpoints/{model_name}\",\n",
    "        filename=\"{model_name}-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        logger=logger,\n",
    "        gradient_clip_val=1.0,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "    \n",
    "    # Train, test với val \n",
    "    trainer.fit(\n",
    "        model, train_dataloaders=training_loader, val_dataloaders=validation_loader\n",
    "    )\n",
    "    trainer.test(model, dataloaders=testing_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = MyModel(input_dim=64, output_dim=2)\n",
    "    train(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
