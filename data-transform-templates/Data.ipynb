{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4c6dab",
   "metadata": {},
   "source": [
    "#### Xoá những ký tự không cần thiết "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db159e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.vi import Vietnamese\n",
    "import re\n",
    "import string\n",
    "\n",
    "nlp = Vietnamese()\n",
    "\n",
    "def text_normalize(text, remove_urls=True, remove_emails=True, remove_mentions=True, remove_hashtags=True, remove_numbers=False):\n",
    "    \"\"\"\n",
    "    Normalize and clean Vietnamese text with additional regex steps:\n",
    "    - remove/normalize URLs, emails, mentions, hashtags, phone numbers, currency symbols\n",
    "    - remove non-printable characters, reduce repeated punctuation and elongated letters\n",
    "    - optional removal of digits\n",
    "    Returns cleaned string (tokens joined by space) after spaCy tokenization and stopword filtering.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = str(text)\n",
    "        # basic normalization\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # remove urls\n",
    "        if remove_urls:\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "        # remove emails\n",
    "        if remove_emails:\n",
    "            text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "        # remove html tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        # mentions (@username)\n",
    "        if remove_mentions:\n",
    "            text = re.sub(r'@\\w+', ' ', text)\n",
    "        # hashtags: keep the word but drop the #\n",
    "        if remove_hashtags:\n",
    "            text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        # remove phone numbers (simple patterns)\n",
    "        text = re.sub(r'\\b0\\d{8,}\\b|\\b\\d{9,}\\b', ' ', text)\n",
    "        # replace common currency symbols with space\n",
    "        text = re.sub(r'[\\$€£¥₫]', ' ', text)\n",
    "\n",
    "        # remove punctuation (translate to space to avoid joining words)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "        # remove non-printable characters\n",
    "        text = ''.join(ch for ch in text if ch.isprintable())\n",
    "\n",
    "        # collapse repeated punctuation (e.g., '!!!' -> '!')\n",
    "        text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "        # reduce elongated characters (>2 repeats -> 2 repeats) e.g. heyyyy -> heyy\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "        # optionally remove digits entirely\n",
    "        if remove_numbers:\n",
    "            text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "        # normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # tokenize with spaCy (Vietnamese) and filter stopwords/punct/space\n",
    "        doc = nlp(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Keep tokens that are not stopwords/punctuation/space\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                # filter out tokens that contain digits (optional already handled above)\n",
    "                if not any(char.isdigit() for char in token.text):\n",
    "                    cleaned_tokens.append(token.lower_)\n",
    "\n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error normalizing text:\", e)\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c990ee",
   "metadata": {},
   "source": [
    "#### Mask LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def MLM(text, mask_prob=0.3):\n",
    "    tokens = text.split()\n",
    "    if not tokens:\n",
    "        return text\n",
    "\n",
    "    num_to_mask = max(1, int(len(tokens) * mask_prob))\n",
    "    mask_indices = random.sample(range(len(tokens)), min(num_to_mask, len(tokens)))\n",
    "\n",
    "    for idx in mask_indices:\n",
    "        tokens[idx] = \"[MASK]\"\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166ba96",
   "metadata": {},
   "source": [
    "#### Cut mix phrase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39092aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def cut_mix_pair(sent1, sent2, tokenizer=None, min_frac=0.2, max_frac=0.8):\n",
    "    \"\"\"\n",
    "    Cắt ghép ngẫu nhiên prefix của sent1 và suffix của sent2.\n",
    "    tokenizer: hàm nhận string trả về list token (mặc định split by whitespace).\n",
    "    min_frac/max_frac: tỉ lệ tối thiểu/tối đa cho vị trí cắt relative to length.\n",
    "    \"\"\"\n",
    "    tok1 = tokenizer(sent1) if tokenizer else sent1.split()\n",
    "    tok2 = tokenizer(sent2) if tokenizer else sent2.split()\n",
    "    if not tok1 or not tok2:\n",
    "        return \"\"\n",
    "    # đảm bảo cut points hợp lệ\n",
    "    lo1 = max(1, int(len(tok1) * min_frac))\n",
    "    hi1 = max(1, int(len(tok1) * max_frac))\n",
    "    lo2 = max(1, int(len(tok2) * min_frac))\n",
    "    hi2 = max(1, int(len(tok2) * max_frac))\n",
    "    cut1 = random.randint(lo1, max(lo1, hi1))\n",
    "    cut2 = random.randint(lo2, max(lo2, hi2))\n",
    "    new_tokens = tok1[:cut1] + tok2[cut2:]\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def augment_cutmix_df(df, text_col='text', label_col='label', n_aug_per_sample=1, tokenizer=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Augment dataframe bằng cách với mỗi nhãn, chọn ngẫu nhiên 2 câu cùng nhãn và tạo câu mới bằng cut_mix.\n",
    "    n_aug_per_sample: số mẫu augmented tạo trên mỗi sample gốc (ước tính, thực tế = n_aug_per_sample * N_label)\n",
    "    Trả về DataFrame gộp data gốc + augmented.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "    augmented = []\n",
    "    for label, group in df.groupby(label_col):\n",
    "        texts = group[text_col].dropna().astype(str).tolist()\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "        # tạo số lượng augment mong muốn dựa trên kích thước group\n",
    "        for _ in range(n_aug_per_sample * len(texts)):\n",
    "            a, b = random.sample(texts, 2)\n",
    "            new_text = cut_mix_pair(a, b, tokenizer=tokenizer)\n",
    "            if new_text:\n",
    "                augmented.append({text_col: new_text, label_col: label})\n",
    "    if not augmented:\n",
    "        return df.copy()\n",
    "    aug_df = pd.DataFrame(augmented)\n",
    "    return pd.concat([df.reset_index(drop=True), aug_df.reset_index(drop=True)], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
