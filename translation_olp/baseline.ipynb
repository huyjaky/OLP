{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "TRAIN_SRC = \"dataset/train/train.zh\"\n",
    "TRAIN_TGT = \"dataset/train/train.vi\"\n",
    "TEST_SRC = \"dataset/private_test/private_test.zh\"\n",
    "SAVE_DIR = \"./checkpoints\"\n",
    "SPM_ZH_PREFIX = os.path.join(SAVE_DIR, \"spm_zh\")\n",
    "SPM_VI_PREFIX = os.path.join(SAVE_DIR, \"spm_vi\")\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 3000\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-2\n",
    "MAX_LEN = 80\n",
    "SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 32061\n",
      "Test samples: 1781\n",
      "\n",
      "Example Chinese sentence: 我 会 给 您 拿 一些 。\n",
      "Example Vietnamese sentence: Tôi sẽ mang cho bạn một ít .\n"
     ]
    }
   ],
   "source": [
    "def read_lines(path):\n",
    "    \"\"\"Read lines from a text file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [l.strip().replace(\"_\", \" \") for l in f if l.strip()]\n",
    "\n",
    "# Load training and test data\n",
    "train_src = read_lines(TRAIN_SRC)\n",
    "train_tgt = read_lines(TRAIN_TGT)\n",
    "test_src = read_lines(TEST_SRC)\n",
    "\n",
    "print(f\"Training samples: {len(train_src)}\")\n",
    "print(f\"Test samples: {len(test_src)}\")\n",
    "print(f\"\\nExample Chinese sentence: {train_src[0]}\")\n",
    "print(f\"Example Vietnamese sentence: {train_tgt[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SentencePiece Tokenization\n",
    "\n",
    "Train BPE tokenizers for both Chinese and Vietnamese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spm(input_file, model_prefix, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"Train a SentencePiece BPE model.\"\"\"\n",
    "    args = (\n",
    "        f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} \"\n",
    "        \"--model_type=bpe --character_coverage=1.0 \"\n",
    "        \"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(args)\n",
    "    print(f\"Trained SentencePiece model: {model_prefix}.model\")\n",
    "\n",
    "def load_sp(model_path):\n",
    "    \"\"\"Load a trained SentencePiece model.\"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chinese vocab size: 3000\n",
      "Vietnamese vocab size: 3000\n",
      "\n",
      "Example tokenization:\n",
      "Original: 我 会 给 您 拿 一些 。\n",
      "Token IDs: [5, 47, 28, 56, 109, 162, 4]...\n"
     ]
    }
   ],
   "source": [
    "# Train Chinese tokenizer\n",
    "tmp_zh = os.path.join(SAVE_DIR, \"tmp_zh.txt\")\n",
    "if not os.path.exists(SPM_ZH_PREFIX + \".model\"):\n",
    "    with open(tmp_zh, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in train_src:\n",
    "            f.write(s + \"\\n\")\n",
    "    train_spm(tmp_zh, SPM_ZH_PREFIX)\n",
    "\n",
    "# Train Vietnamese tokenizer\n",
    "tmp_vi = os.path.join(SAVE_DIR, \"tmp_vi.txt\")\n",
    "if not os.path.exists(SPM_VI_PREFIX + \".model\"):\n",
    "    with open(tmp_vi, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in train_tgt:\n",
    "            f.write(s + \"\\n\")\n",
    "    train_spm(tmp_vi, SPM_VI_PREFIX)\n",
    "\n",
    "# Load tokenizers\n",
    "sp_zh = load_sp(SPM_ZH_PREFIX + \".model\")\n",
    "sp_vi = load_sp(SPM_VI_PREFIX + \".model\")\n",
    "\n",
    "print(f\"\\nChinese vocab size: {sp_zh.GetPieceSize()}\")\n",
    "print(f\"Vietnamese vocab size: {sp_vi.GetPieceSize()}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_sent = train_src[0]\n",
    "tokens = sp_zh.EncodeAsIds(test_sent)\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"Original: {test_sent}\")\n",
    "print(f\"Token IDs: {tokens[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset for Chinese-Vietnamese translation pairs.\"\"\"\n",
    "    \n",
    "    def __init__(self, src, tgt, sp_src, sp_tgt, max_len=MAX_LEN):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        self.sp_src = sp_src\n",
    "        self.sp_tgt = sp_tgt\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Add BOS (2) and EOS (3) tokens\n",
    "        src_ids = [2] + self.sp_src.EncodeAsIds(self.src[idx])[:self.max_len-2] + [3]\n",
    "        tgt_ids = [2] + self.sp_tgt.EncodeAsIds(self.tgt[idx])[:self.max_len-2] + [3]\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences to the same length.\"\"\"\n",
    "    srcs, tgts = zip(*batch)\n",
    "    max_src = max(len(s) for s in srcs)\n",
    "    max_tgt = max(len(t) for t in tgts)\n",
    "    \n",
    "    # Pad with 0 (PAD token)\n",
    "    src_pad = torch.zeros(len(batch), max_src, dtype=torch.long)\n",
    "    tgt_pad = torch.zeros(len(batch), max_tgt, dtype=torch.long)\n",
    "    \n",
    "    for i, (s, t) in enumerate(zip(srcs, tgts)):\n",
    "        src_pad[i, :len(s)] = s\n",
    "        tgt_pad[i, :len(t)] = t\n",
    "    \n",
    "    return src_pad, tgt_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 32061\n",
      "Training samples: 31740\n",
      "Validation samples: 321\n",
      "Training batches: 496\n",
      "Validation batches: 11\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset and dataloader\n",
    "# Split data: 90% train, 10% validation\n",
    "total_samples = len(train_src)\n",
    "train_size = int(0.99 * total_samples)\n",
    "\n",
    "train_src_split = train_src[:train_size]\n",
    "train_tgt_split = train_tgt[:train_size]\n",
    "valid_src = train_src[train_size:]\n",
    "valid_tgt = train_tgt[train_size:]\n",
    "\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Training samples: {len(train_src_split)}\")\n",
    "print(f\"Validation samples: {len(valid_src)}\")\n",
    "\n",
    "dataset = TranslationDataset(train_src_split, train_tgt_split, sp_zh, sp_vi)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "valid_dataset = TranslationDataset(valid_src, valid_tgt, sp_zh, sp_vi)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Training batches: {len(dataloader)}\")\n",
    "print(f\"Validation batches: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n",
    "\n",
    "### Encoder-Decoder with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(\n",
    "            x + self.pe[:, : x.size(1), :]\n",
    "        )  \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"GRU-based encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.m_attn = nn.MultiheadAttention(emb_size, num_heads=16, dropout=0.1, batch_first=True)\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.pe = PositionalEncoding(emb_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # emb = self.pe(self.embedding(src))\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        emb = self.m_attn(emb, emb, emb)[0] # apply multi-head attention\n",
    "        _, (hidden, cell) = self.rnn(emb)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"GRU-based decoder with teacherj forcing.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # emb_size = emb_size * 2\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.batch_norm = nn.BatchNorm1d(emb_size)\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, input_step, hidden, cell):\n",
    "        # input_step = self.batch_norm(input_step)\n",
    "        emb = self.dropout(self.embedding(input_step))\n",
    "        output, (hidden, cell) = self.rnn(emb, (hidden, cell))\n",
    "        pred = self.fc(output.squeeze(1))\n",
    "        return pred, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.3):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # Store outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
    "\n",
    "        # Encode source sentence\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # Start with BOS token\n",
    "        input_step = tgt[:, 0].unsqueeze(1)\n",
    "\n",
    "        # Decode step by step\n",
    "        for t in range(1, tgt_len):\n",
    "            pred, hidden, cell = self.decoder(input_step, hidden, cell)\n",
    "            outputs[:, t] = pred\n",
    "\n",
    "            # Teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # input_step = (\n",
    "            #     tgt[:, t].unsqueeze(1) if teacher_force else pred.argmax(1).unsqueeze(1)\n",
    "            # )\n",
    "            input_step = (\n",
    "                pred.argmax(1).unsqueeze(1)\n",
    "            )         \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1,569,080 trainable parameters\n",
      "\n",
      "Model architecture:\n",
      "Seq2Seq(\n",
      "  (encoder): EncoderRNN(\n",
      "    (embedding): Embedding(3000, 64, padding_idx=0)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (m_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (rnn): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "    (pe): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embedding): Embedding(3000, 64, padding_idx=0)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (rnn): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=3000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "encoder = EncoderRNN(sp_zh.GetPieceSize(), EMB_SIZE, HID_SIZE)\n",
    "decoder = DecoderRNN(sp_vi.GetPieceSize(), EMB_SIZE, HID_SIZE)\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Calculate loss (ignore first BOS token)\n",
    "        loss = criterion(\n",
    "            output[:, 1:].reshape(-1, output.size(-1)), \n",
    "            tgt[:, 1:].reshape(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bleu(model, dataloader, sp_tgt):\n",
    "    \"\"\"Evaluate model using SacreBLEU metric.\"\"\"\n",
    "    model.eval()\n",
    "    hyps, refs = [], []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    for src, tgt in pbar:\n",
    "        src = src.to(DEVICE)\n",
    "        \n",
    "        # Encode\n",
    "        hidden, cell = model.encoder(src)\n",
    "        \n",
    "        # Decode (greedy)\n",
    "        input_step = torch.full((src.size(0), 1), 2, dtype=torch.long, device=DEVICE)\n",
    "        decoded = [[] for _ in range(src.size(0))]\n",
    "        \n",
    "        for _ in range(MAX_LEN):\n",
    "            pred, hidden, cell = model.decoder(input_step, hidden, cell)\n",
    "            next_token = pred.argmax(1).unsqueeze(1)\n",
    "            \n",
    "            for i in range(src.size(0)):\n",
    "                decoded[i].append(next_token[i].item())\n",
    "            \n",
    "            input_step = next_token\n",
    "        \n",
    "        # Convert to text\n",
    "        for i in range(src.size(0)):\n",
    "            ids = decoded[i]\n",
    "            if 3 in ids:  # Stop at EOS\n",
    "                ids = ids[:ids.index(3)]\n",
    "            hyps.append(sp_tgt.DecodeIds(ids))\n",
    "            \n",
    "            ref_ids = tgt[i].tolist()[1:-1]  # Remove BOS and EOS\n",
    "            refs.append(sp_tgt.DecodeIds([x for x in ref_ids if x not in [0, 1]]))\n",
    "    \n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss=5.390 | SacreBLEU=0.20 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Loss=5.139 | SacreBLEU=0.36 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Loss=5.076 | SacreBLEU=0.42 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Loss=5.055 | SacreBLEU=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Loss=5.052 | SacreBLEU=0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Loss=5.110 | SacreBLEU=0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Loss=5.222 | SacreBLEU=0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Loss=5.158 | SacreBLEU=0.42 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Loss=5.120 | SacreBLEU=0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss=5.103 | SacreBLEU=0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss=5.074 | SacreBLEU=0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss=5.068 | SacreBLEU=0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss=5.032 | SacreBLEU=0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss=5.042 | SacreBLEU=0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Loss=5.020 | SacreBLEU=0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Loss=5.012 | SacreBLEU=0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Loss=4.998 | SacreBLEU=0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Loss=4.992 | SacreBLEU=0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Loss=4.982 | SacreBLEU=0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Loss=4.980 | SacreBLEU=0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Loss=4.955 | SacreBLEU=0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Loss=4.954 | SacreBLEU=0.44 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Loss=4.981 | SacreBLEU=0.59 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Loss=4.986 | SacreBLEU=0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Loss=4.990 | SacreBLEU=0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Loss=5.004 | SacreBLEU=0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Loss=4.987 | SacreBLEU=0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Loss=4.962 | SacreBLEU=0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Loss=5.013 | SacreBLEU=0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Loss=5.015 | SacreBLEU=0.32\n",
      "\n",
      "Training completed!\n",
      "Best validation BLEU: 0.59\n",
      "Best model saved to: ./checkpoints/best_model.pt\n",
      "Loaded best model from epoch 23\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Training loop with best model saving\n",
    "best_bleu = 0.0\n",
    "best_model_path = os.path.join(SAVE_DIR, \"best_model.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_epoch(model, dataloader, criterion, optimizer)\n",
    "    bleu = evaluate_bleu(model, valid_loader, sp_vi)\n",
    "    \n",
    "    # Save best model\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'bleu': bleu,\n",
    "            'loss': loss\n",
    "        }, best_model_path)\n",
    "        print(f\"Epoch {epoch:02d} | Loss={loss:.3f} | SacreBLEU={bleu:.2f} Best model saved!\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:02d} | Loss={loss:.3f} | SacreBLEU={bleu:.2f}\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation BLEU: {best_bleu:.2f}\")\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Load best model for inference\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Translation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate_test(model, test_src, sp_src, sp_tgt, out_path):\n",
    "    \"\"\"Translate test set and save to CSV.\"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    \n",
    "    for s in tqdm(test_src, desc=\"Translating\"):\n",
    "        # Tokenize source sentence\n",
    "        src_ids = [2] + sp_src.EncodeAsIds(s)[:MAX_LEN-2] + [3]\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "        \n",
    "        # Encode\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Decode\n",
    "        input_step = torch.full((1, 1), 2, dtype=torch.long, device=DEVICE)\n",
    "        decoded = []\n",
    "        \n",
    "        for _ in range(MAX_LEN):\n",
    "            pred, hidden, cell = model.decoder(input_step, hidden, cell)\n",
    "            next_token = pred.argmax(1)\n",
    "            token = next_token.item()\n",
    "            \n",
    "            if token == 3:  # EOS\n",
    "                break\n",
    "            \n",
    "            decoded.append(token)\n",
    "            input_step = next_token.unsqueeze(1)\n",
    "        \n",
    "        # Decode to Vietnamese text\n",
    "        vi_sent = sp_tgt.DecodeIds(decoded)\n",
    "        outputs.append(vi_sent)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame({\"tieng_trung\": test_src, \"tieng_viet\": outputs})\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 1781/1781 [00:04<00:00, 375.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed!\n",
      "Saved to: private_test.csv\n",
      "\n",
      "Sample translations:\n",
      "                                    tieng_trung  \\\n",
      "0                                   我 会 呆 两 天 。   \n",
      "1                                   我 现在 在 机场 。   \n",
      "2                                 玛丽 不 比 亨利 大 。   \n",
      "3                                 这些 问题 并不 新鲜 。   \n",
      "4                              再 说 一 次 你 的 姓名 。   \n",
      "5                                我 大概 有 三千 美元 。   \n",
      "6                     当然 了 先生 , 可是 你 出 什么 事 了 ？   \n",
      "7                                     你 需要 活动 。   \n",
      "8                                   它 在 星期几 开 ？   \n",
      "9  你 知道 日本 的 摔跤 传统 , 在 相扑 冠军 之间 有 一 个 是 美国人 吗 ？   \n",
      "\n",
      "                             tieng_viet  \n",
      "0          Tôi muốn tôi một một một . .  \n",
      "1        Tôi muốn mua một một . . . . .  \n",
      "2                     Chúng ta là là là  \n",
      "3  Bạn có , bạn tôi tôi tôi . . . . . .  \n",
      "4         Bạn , , , , tôi , tôi . . . .  \n",
      "5  Tôi muốn thể tôi bạn bạn bạn . . . .  \n",
      "6                        được , , , , ,  \n",
      "7          Bạn có thể bạn bạn bạn bạn .  \n",
      "8                     Nó ta thể tôi tôi  \n",
      "9                    Bạn có , , , , , ,  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate test set\n",
    "submission_path = os.path.join(\"private_test.csv\")\n",
    "translate_test(model, test_src, sp_zh, sp_vi, submission_path)\n",
    "\n",
    "print(f\"Translation completed!\")\n",
    "print(f\"Saved to: {submission_path}\")\n",
    "\n",
    "# Display sample translations\n",
    "df_result = pd.read_csv(submission_path)\n",
    "print(\"\\nSample translations:\")\n",
    "print(df_result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Submission ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission ZIP created: public_submission.zip\n",
      "File size: 35.31 KB\n"
     ]
    }
   ],
   "source": [
    "# Create ZIP file for submission\n",
    "zip_path = os.path.join(\"public_submission.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(submission_path, arcname=\"private_test.csv\")\n",
    "\n",
    "print(f\"Submission ZIP created: {zip_path}\")\n",
    "print(f\"File size: {os.path.getsize(zip_path) / 1024:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
