{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "TRAIN_SRC = \"dataset/train/train.zh\"\n",
    "TRAIN_TGT = \"dataset/train/train.vi\"\n",
    "TEST_SRC = \"dataset/private_test/private_test.zh\"\n",
    "SAVE_DIR = \"./checkpoints\"\n",
    "SPM_ZH_PREFIX = os.path.join(SAVE_DIR, \"spm_zh\")\n",
    "SPM_VI_PREFIX = os.path.join(SAVE_DIR, \"spm_vi\")\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 3000\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-2\n",
    "MAX_LEN = 80\n",
    "SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 32061\n",
      "Test samples: 1781\n",
      "\n",
      "Example Chinese sentence: 我 会 给 您 拿 一些 。\n",
      "Example Vietnamese sentence: Tôi sẽ mang cho bạn một ít .\n"
     ]
    }
   ],
   "source": [
    "def read_lines(path):\n",
    "    \"\"\"Read lines from a text file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [l.strip().replace(\"_\", \" \") for l in f if l.strip()]\n",
    "\n",
    "# Load training and test data\n",
    "train_src = read_lines(TRAIN_SRC)\n",
    "train_tgt = read_lines(TRAIN_TGT)\n",
    "test_src = read_lines(TEST_SRC)\n",
    "\n",
    "print(f\"Training samples: {len(train_src)}\")\n",
    "print(f\"Test samples: {len(test_src)}\")\n",
    "print(f\"\\nExample Chinese sentence: {train_src[0]}\")\n",
    "print(f\"Example Vietnamese sentence: {train_tgt[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SentencePiece Tokenization\n",
    "\n",
    "Train BPE tokenizers for both Chinese and Vietnamese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spm(input_file, model_prefix, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"Train a SentencePiece BPE model.\"\"\"\n",
    "    args = (\n",
    "        f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} \"\n",
    "        \"--model_type=bpe --character_coverage=1.0 \"\n",
    "        \"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(args)\n",
    "    print(f\"Trained SentencePiece model: {model_prefix}.model\")\n",
    "\n",
    "def load_sp(model_path):\n",
    "    \"\"\"Load a trained SentencePiece model.\"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained SentencePiece model: ./checkpoints/spm_zh.model\n",
      "Trained SentencePiece model: ./checkpoints/spm_vi.model\n",
      "\n",
      "Chinese vocab size: 3000\n",
      "Vietnamese vocab size: 3000\n",
      "\n",
      "Example tokenization:\n",
      "Original: 我 会 给 您 拿 一些 。\n",
      "Token IDs: [5, 47, 28, 56, 109, 162, 4]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./checkpoints/tmp_zh.txt --model_prefix=./checkpoints/spm_zh --vocab_size=3000 --model_type=bpe --character_coverage=1.0 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./checkpoints/tmp_zh.txt\n",
      "  input_format: \n",
      "  model_prefix: ./checkpoints/spm_zh\n",
      "  model_type: BPE\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./checkpoints/tmp_zh.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 32061 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=562242\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=2778\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 32061 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 32061\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 12433\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18099 min_freq=35\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2055 size=20 all=17060 active=1238 piece=▁好\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=932 size=40 all=17368 active=1546 piece=▁和\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=592 size=60 all=17710 active=1888 piece=▁对\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=499 size=80 all=17934 active=2112 piece=▁喜欢\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=444 size=100 all=18147 active=2325 piece=▁明\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=444 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=381 size=120 all=18349 active=1185 piece=▁找\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=335 size=140 all=18500 active=1336 piece=▁预\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=288 size=160 all=18742 active=1578 piece=▁需要\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=251 size=180 all=18820 active=1656 piece=▁但是\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=229 size=200 all=18981 active=1817 piece=▁条\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=229 min_freq=29\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ./checkpoints/spm_zh.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ./checkpoints/spm_zh.vocab\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./checkpoints/tmp_vi.txt --model_prefix=./checkpoints/spm_vi --vocab_size=3000 --model_type=bpe --character_coverage=1.0 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./checkpoints/tmp_vi.txt\n",
      "  input_format: \n",
      "  model_prefix: ./checkpoints/spm_vi\n",
      "  model_type: BPE\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./checkpoints/tmp_vi.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 32061 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=1210277\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=177\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 32061 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 32061\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 5892\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38944 min_freq=31\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8120 size=20 all=3432 active=1804 piece=▁g\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3803 size=40 all=3915 active=2287 piece=ột\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2504 size=60 all=4184 active=2556 piece=ợc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2040 size=80 all=4419 active=2791 piece=ới\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1444 size=100 all=4715 active=3087 piece=▁L\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1437 min_freq=53\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1179 size=120 all=4978 active=1218 piece=▁vui\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=976 size=140 all=5157 active=1397 piece=êu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=866 size=160 all=5393 active=1633 piece=ân\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=769 size=180 all=5531 active=1771 piece=▁để\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=700 size=200 all=5688 active=1928 piece=▁nay\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=698 min_freq=49\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=639 size=220 all=5822 active=1135 piece=▁gọi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=561 size=240 all=5976 active=1289 piece=ết\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=517 size=260 all=6140 active=1453 piece=▁đặt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=465 size=280 all=6243 active=1556 piece=ỏi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=430 size=300 all=6343 active=1656 piece=▁đô\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=425 min_freq=43\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=392 size=320 all=6430 active=1086 piece=▁đầu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=360 size=340 all=6487 active=1143 piece=▁sạn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=338 size=360 all=6510 active=1166 piece=▁hỏi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=318 size=380 all=6627 active=1283 piece=âng\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=400 all=6762 active=1418 piece=▁sinh\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=297 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=278 size=420 all=6842 active=1081 piece=en\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=260 size=440 all=6981 active=1220 piece=ươi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=240 size=460 all=7019 active=1258 piece=ừa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=480 all=7051 active=1290 piece=▁phố\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=204 size=500 all=7120 active=1359 piece=▁thêm\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=202 min_freq=31\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=520 all=7136 active=1017 piece=▁tiếp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=540 all=7166 active=1047 piece=ầm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=560 all=7232 active=1113 piece=▁đề\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=580 all=7313 active=1194 piece=▁vụ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=600 all=7416 active=1297 piece=▁7\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=157 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=620 all=7492 active=1067 piece=▁hiểu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=640 all=7517 active=1092 piece=▁hộ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=138 size=660 all=7569 active=1144 piece=▁Khi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=680 all=7611 active=1186 piece=▁bánh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=700 all=7624 active=1199 piece=▁sống\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=124 min_freq=23\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=720 all=7671 active=1048 piece=▁xong\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=740 all=7683 active=1060 piece=▁trạm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=760 all=7740 active=1117 piece=▁Mỹ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105 size=780 all=7776 active=1153 piece=óa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=101 size=800 all=7800 active=1177 piece=ur\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=101 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96 size=820 all=7873 active=1046 piece=▁phục\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=840 all=7905 active=1078 piece=▁Hôm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=860 all=7930 active=1103 piece=▁đông\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=880 all=7977 active=1150 piece=▁thuế\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=900 all=8027 active=1200 piece=ưỡ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=82 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=920 all=8034 active=1001 piece=ùm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=940 all=8071 active=1038 piece=▁đóng\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=960 all=8063 active=1030 piece=▁lễ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=980 all=8099 active=1066 piece=▁Mấy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=1000 all=8107 active=1074 piece=▁xách\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=67 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=1020 all=8178 active=1072 piece=ặn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=1040 all=8172 active=1066 piece=▁đăng\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=1060 all=8198 active=1092 piece=▁11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=1080 all=8215 active=1109 piece=▁miễn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=1100 all=8230 active=1124 piece=▁ngữ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=57 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=1120 all=8238 active=1009 piece=▁Vì\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=1140 all=8252 active=1023 piece=▁lực\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=1160 all=8247 active=1018 piece=ưỡi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=1180 all=8265 active=1036 piece=▁tương\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=1200 all=8287 active=1058 piece=▁sáo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=48 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=1220 all=8267 active=981 piece=▁đen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=1240 all=8265 active=979 piece=▁Hải\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=1260 all=8251 active=965 piece=▁rảnh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=1280 all=8257 active=971 piece=▁Máy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=1300 all=8280 active=994 piece=▁gắng\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=40 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=1320 all=8292 active=1013 piece=▁Nhân\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1340 all=8289 active=1010 piece=isco\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1360 all=8304 active=1025 piece=▁xà\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1380 all=8298 active=1019 piece=▁mãi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1400 all=8291 active=1012 piece=▁hãng\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=33 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1420 all=8331 active=1041 piece=▁Trần\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1440 all=8342 active=1052 piece=▁thao\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1460 all=8355 active=1065 piece=▁lương\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1480 all=8355 active=1065 piece=▁cưới\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1500 all=8345 active=1055 piece=▁bụng\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1520 all=8373 active=1029 piece=▁ống\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1540 all=8377 active=1033 piece=▁Nơi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1560 all=8404 active=1060 piece=▁Mi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1580 all=8400 active=1056 piece=▁miếng\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1600 all=8395 active=1051 piece=▁sâu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1620 all=8392 active=998 piece=▁In\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1640 all=8379 active=985 piece=▁Đúng\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1660 all=8458 active=1064 piece=▁tỉ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1680 all=8439 active=1045 piece=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1700 all=8518 active=1124 piece=▁tắc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=21 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1720 all=8522 active=1005 piece=òi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1740 all=8538 active=1021 piece=▁dâu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1760 all=8521 active=1004 piece=▁khắc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1780 all=8532 active=1015 piece=▁xét\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1800 all=8515 active=998 piece=ky\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1820 all=8547 active=1028 piece=▁cắm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1840 all=8528 active=1009 piece=▁Washington\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1860 all=8571 active=1052 piece=▁hoà\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1880 all=8552 active=1033 piece=alifor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1900 all=8593 active=1074 piece=▁xế\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1920 all=8596 active=1004 piece=▁đàm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1940 all=8577 active=985 piece=▁đích\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1960 all=8632 active=1040 piece=▁Kỳ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1980 all=8630 active=1038 piece=▁đáo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2000 all=8625 active=1033 piece=ie\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2020 all=8647 active=1014 piece=▁Thẻ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2040 all=8627 active=994 piece=▁chứa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2060 all=8608 active=975 piece=ir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2080 all=8635 active=1002 piece=▁Dân\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2100 all=8621 active=988 piece=▁tuỳ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2120 all=8602 active=982 piece=ab\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2140 all=8655 active=1035 piece=▁Ôi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2160 all=8645 active=1025 piece=▁nho\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2180 all=8630 active=1010 piece=▁khẩn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2200 all=8610 active=990 piece=▁trạng\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2220 all=8676 active=1067 piece=▁72\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2240 all=8665 active=1056 piece=▁cơn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2260 all=8645 active=1036 piece=▁Giáo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2280 all=8629 active=1020 piece=▁Thanh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2300 all=8673 active=1064 piece=ame\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2320 all=8723 active=1048 piece=▁hô\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2340 all=8721 active=1046 piece=▁Sài\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2360 all=8708 active=1033 piece=▁náo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2380 all=8690 active=1015 piece=▁kênh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2400 all=8671 active=996 piece=▁United\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2420 all=8729 active=1059 piece=▁65\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2440 all=8728 active=1058 piece=▁Car\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2460 all=8718 active=1048 piece=▁hoạ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2480 all=8701 active=1031 piece=awaii\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2500 all=8684 active=1014 piece=▁Kyoto\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2520 all=8680 active=997 piece=567\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2540 all=8701 active=1018 piece=▁Vi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2560 all=8704 active=1021 piece=▁Gặp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2580 all=8696 active=1013 piece=▁mèo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2600 all=8679 active=996 piece=▁Jane\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2620 all=8660 active=982 piece=▁suối\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=2640 all=8644 active=966 piece=▁khiếu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2660 all=8687 active=1009 piece=ion\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2680 all=8701 active=1023 piece=▁hỗ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2700 all=8716 active=1038 piece=▁bực\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2720 all=8698 active=983 piece=▁qui\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2740 all=8686 active=971 piece=▁Hiệu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2760 all=8668 active=953 piece=▁giật\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=2780 all=8651 active=936 piece=▁Thiết\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=2800 all=8641 active=926 piece=56\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=3\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ./checkpoints/spm_vi.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ./checkpoints/spm_vi.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train Chinese tokenizer\n",
    "tmp_zh = os.path.join(SAVE_DIR, \"tmp_zh.txt\")\n",
    "if not os.path.exists(SPM_ZH_PREFIX + \".model\"):\n",
    "    with open(tmp_zh, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in train_src:\n",
    "            f.write(s + \"\\n\")\n",
    "    train_spm(tmp_zh, SPM_ZH_PREFIX)\n",
    "\n",
    "# Train Vietnamese tokenizer\n",
    "tmp_vi = os.path.join(SAVE_DIR, \"tmp_vi.txt\")\n",
    "if not os.path.exists(SPM_VI_PREFIX + \".model\"):\n",
    "    with open(tmp_vi, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in train_tgt:\n",
    "            f.write(s + \"\\n\")\n",
    "    train_spm(tmp_vi, SPM_VI_PREFIX)\n",
    "\n",
    "# Load tokenizers\n",
    "sp_zh = load_sp(SPM_ZH_PREFIX + \".model\")\n",
    "sp_vi = load_sp(SPM_VI_PREFIX + \".model\")\n",
    "\n",
    "print(f\"\\nChinese vocab size: {sp_zh.GetPieceSize()}\")\n",
    "print(f\"Vietnamese vocab size: {sp_vi.GetPieceSize()}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_sent = train_src[0]\n",
    "tokens = sp_zh.EncodeAsIds(test_sent)\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"Original: {test_sent}\")\n",
    "print(f\"Token IDs: {tokens[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset for Chinese-Vietnamese translation pairs.\"\"\"\n",
    "    \n",
    "    def __init__(self, src, tgt, sp_src, sp_tgt, max_len=MAX_LEN):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        self.sp_src = sp_src\n",
    "        self.sp_tgt = sp_tgt\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Add BOS (2) and EOS (3) tokens\n",
    "        src_ids = [2] + self.sp_src.EncodeAsIds(self.src[idx])[:self.max_len-2] + [3]\n",
    "        tgt_ids = [2] + self.sp_tgt.EncodeAsIds(self.tgt[idx])[:self.max_len-2] + [3]\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences to the same length.\"\"\"\n",
    "    srcs, tgts = zip(*batch)\n",
    "    max_src = max(len(s) for s in srcs)\n",
    "    max_tgt = max(len(t) for t in tgts)\n",
    "    \n",
    "    # Pad with 0 (PAD token)\n",
    "    src_pad = torch.zeros(len(batch), max_src, dtype=torch.long)\n",
    "    tgt_pad = torch.zeros(len(batch), max_tgt, dtype=torch.long)\n",
    "    \n",
    "    for i, (s, t) in enumerate(zip(srcs, tgts)):\n",
    "        src_pad[i, :len(s)] = s\n",
    "        tgt_pad[i, :len(t)] = t\n",
    "    \n",
    "    return src_pad, tgt_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 32061\n",
      "Training samples: 31740\n",
      "Validation samples: 321\n",
      "Training batches: 496\n",
      "Validation batches: 11\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset and dataloader\n",
    "# Split data: 90% train, 10% validation\n",
    "total_samples = len(train_src)\n",
    "train_size = int(0.99 * total_samples)\n",
    "\n",
    "train_src_split = train_src[:train_size]\n",
    "train_tgt_split = train_tgt[:train_size]\n",
    "valid_src = train_src[train_size:]\n",
    "valid_tgt = train_tgt[train_size:]\n",
    "\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Training samples: {len(train_src_split)}\")\n",
    "print(f\"Validation samples: {len(valid_src)}\")\n",
    "\n",
    "dataset = TranslationDataset(train_src_split, train_tgt_split, sp_zh, sp_vi)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "valid_dataset = TranslationDataset(valid_src, valid_tgt, sp_zh, sp_vi)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Training batches: {len(dataloader)}\")\n",
    "print(f\"Validation batches: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n",
    "\n",
    "### Encoder-Decoder with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(\n",
    "            x + self.pe[:, : x.size(1), :]\n",
    "        )  \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"GRU-based encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.m_attn = nn.MultiheadAttention(emb_size, num_heads=16, dropout=0.1, batch_first=True)\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.pe = PositionalEncoding(emb_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # emb = self.pe(self.embedding(src))\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        emb = self.m_attn(emb, emb, emb)[0] # apply multi-head attention\n",
    "        _, (hidden, cell) = self.rnn(emb)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"GRU-based decoder with teacherj forcing.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # emb_size = emb_size * 2\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.batch_norm = nn.BatchNorm1d(emb_size)\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, input_step, hidden, cell):\n",
    "        # input_step = self.batch_norm(input_step)\n",
    "        emb = self.dropout(self.embedding(input_step))\n",
    "        output, (hidden, cell) = self.rnn(emb, (hidden, cell))\n",
    "        pred = self.fc(output.squeeze(1))\n",
    "        return pred, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.3):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # Store outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
    "\n",
    "        # Encode source sentence\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # Start with BOS token\n",
    "        input_step = tgt[:, 0].unsqueeze(1)\n",
    "\n",
    "        # Decode step by step\n",
    "        for t in range(1, tgt_len):\n",
    "            pred, hidden, cell = self.decoder(input_step, hidden, cell)\n",
    "            outputs[:, t] = pred\n",
    "\n",
    "            # Teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # input_step = (\n",
    "            #     tgt[:, t].unsqueeze(1) if teacher_force else pred.argmax(1).unsqueeze(1)\n",
    "            # )\n",
    "            input_step = (\n",
    "                pred.argmax(1).unsqueeze(1)\n",
    "            )         \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1,569,080 trainable parameters\n",
      "\n",
      "Model architecture:\n",
      "Seq2Seq(\n",
      "  (encoder): EncoderRNN(\n",
      "    (embedding): Embedding(3000, 64, padding_idx=0)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (m_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (rnn): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "    (pe): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embedding): Embedding(3000, 64, padding_idx=0)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "    (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (rnn): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=3000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "encoder = EncoderRNN(sp_zh.GetPieceSize(), EMB_SIZE, HID_SIZE)\n",
    "decoder = DecoderRNN(sp_vi.GetPieceSize(), EMB_SIZE, HID_SIZE)\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Calculate loss (ignore first BOS token)\n",
    "        loss = criterion(\n",
    "            output[:, 1:].reshape(-1, output.size(-1)), \n",
    "            tgt[:, 1:].reshape(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bleu(model, dataloader, sp_tgt):\n",
    "    \"\"\"Evaluate model using SacreBLEU metric.\"\"\"\n",
    "    model.eval()\n",
    "    hyps, refs = [], []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    for src, tgt in pbar:\n",
    "        src = src.to(DEVICE)\n",
    "        \n",
    "        # Encode\n",
    "        hidden, cell = model.encoder(src)\n",
    "        \n",
    "        # Decode (greedy)\n",
    "        input_step = torch.full((src.size(0), 1), 2, dtype=torch.long, device=DEVICE)\n",
    "        decoded = [[] for _ in range(src.size(0))]\n",
    "        \n",
    "        for _ in range(MAX_LEN):\n",
    "            pred, hidden, cell = model.decoder(input_step, hidden, cell)\n",
    "            next_token = pred.argmax(1).unsqueeze(1)\n",
    "            \n",
    "            for i in range(src.size(0)):\n",
    "                decoded[i].append(next_token[i].item())\n",
    "            \n",
    "            input_step = next_token\n",
    "        \n",
    "        # Convert to text\n",
    "        for i in range(src.size(0)):\n",
    "            ids = decoded[i]\n",
    "            if 3 in ids:  # Stop at EOS\n",
    "                ids = ids[:ids.index(3)]\n",
    "            hyps.append(sp_tgt.DecodeIds(ids))\n",
    "            \n",
    "            ref_ids = tgt[i].tolist()[1:-1]  # Remove BOS and EOS\n",
    "            refs.append(sp_tgt.DecodeIds([x for x in ref_ids if x not in [0, 1]]))\n",
    "    \n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss=5.374 | SacreBLEU=0.44 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Loss=5.105 | SacreBLEU=0.63 Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')      \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Loss=5.048 | SacreBLEU=0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Loss=5.040 | SacreBLEU=0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Loss=5.038 | SacreBLEU=0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')     \n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Loss=5.040 | SacreBLEU=0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/olp/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/venv/olp/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/venv/olp/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 52, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/venv/olp/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/venv/olp/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/venv/olp/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/venv/olp/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/venv/olp/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/venv/olp/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/venv/olp/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     bleu \u001b[38;5;241m=\u001b[39m evaluate_bleu(model, valid_loader, sp_vi)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Save best model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate loss (ignore first BOS token)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     14\u001b[0m     output[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \n\u001b[1;32m     15\u001b[0m     tgt[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/venv/olp/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/olp/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/olp/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Training loop with best model saving\n",
    "best_bleu = 0.0\n",
    "best_model_path = os.path.join(SAVE_DIR, \"best_model.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_epoch(model, dataloader, criterion, optimizer)\n",
    "    bleu = evaluate_bleu(model, valid_loader, sp_vi)\n",
    "    \n",
    "    # Save best model\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'bleu': bleu,\n",
    "            'loss': loss\n",
    "        }, best_model_path)\n",
    "        print(f\"Epoch {epoch:02d} | Loss={loss:.3f} | SacreBLEU={bleu:.2f} Best model saved!\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:02d} | Loss={loss:.3f} | SacreBLEU={bleu:.2f}\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation BLEU: {best_bleu:.2f}\")\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Load best model for inference\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Translation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate_test(model, test_src, sp_src, sp_tgt, out_path):\n",
    "    \"\"\"Translate test set and save to CSV.\"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    \n",
    "    for s in tqdm(test_src, desc=\"Translating\"):\n",
    "        # Tokenize source sentence\n",
    "        src_ids = [2] + sp_src.EncodeAsIds(s)[:MAX_LEN-2] + [3]\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "        \n",
    "        # Encode\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Decode\n",
    "        input_step = torch.full((1, 1), 2, dtype=torch.long, device=DEVICE)\n",
    "        decoded = []\n",
    "        \n",
    "        for _ in range(MAX_LEN):\n",
    "            pred, hidden, cell = model.decoder(input_step, hidden, cell)\n",
    "            next_token = pred.argmax(1)\n",
    "            token = next_token.item()\n",
    "            \n",
    "            if token == 3:  # EOS\n",
    "                break\n",
    "            \n",
    "            decoded.append(token)\n",
    "            input_step = next_token.unsqueeze(1)\n",
    "        \n",
    "        # Decode to Vietnamese text\n",
    "        vi_sent = sp_tgt.DecodeIds(decoded)\n",
    "        outputs.append(vi_sent)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame({\"tieng_trung\": test_src, \"tieng_viet\": outputs})\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 1781/1781 [00:04<00:00, 375.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed!\n",
      "Saved to: private_test.csv\n",
      "\n",
      "Sample translations:\n",
      "                                    tieng_trung  \\\n",
      "0                                   我 会 呆 两 天 。   \n",
      "1                                   我 现在 在 机场 。   \n",
      "2                                 玛丽 不 比 亨利 大 。   \n",
      "3                                 这些 问题 并不 新鲜 。   \n",
      "4                              再 说 一 次 你 的 姓名 。   \n",
      "5                                我 大概 有 三千 美元 。   \n",
      "6                     当然 了 先生 , 可是 你 出 什么 事 了 ？   \n",
      "7                                     你 需要 活动 。   \n",
      "8                                   它 在 星期几 开 ？   \n",
      "9  你 知道 日本 的 摔跤 传统 , 在 相扑 冠军 之间 有 一 个 是 美国人 吗 ？   \n",
      "\n",
      "                             tieng_viet  \n",
      "0          Tôi muốn tôi một một một . .  \n",
      "1        Tôi muốn mua một một . . . . .  \n",
      "2                     Chúng ta là là là  \n",
      "3  Bạn có , bạn tôi tôi tôi . . . . . .  \n",
      "4         Bạn , , , , tôi , tôi . . . .  \n",
      "5  Tôi muốn thể tôi bạn bạn bạn . . . .  \n",
      "6                        được , , , , ,  \n",
      "7          Bạn có thể bạn bạn bạn bạn .  \n",
      "8                     Nó ta thể tôi tôi  \n",
      "9                    Bạn có , , , , , ,  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate test set\n",
    "submission_path = os.path.join(\"private_test.csv\")\n",
    "translate_test(model, test_src, sp_zh, sp_vi, submission_path)\n",
    "\n",
    "print(f\"Translation completed!\")\n",
    "print(f\"Saved to: {submission_path}\")\n",
    "\n",
    "# Display sample translations\n",
    "df_result = pd.read_csv(submission_path)\n",
    "print(\"\\nSample translations:\")\n",
    "print(df_result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Submission ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission ZIP created: public_submission.zip\n",
      "File size: 35.31 KB\n"
     ]
    }
   ],
   "source": [
    "# Create ZIP file for submission\n",
    "zip_path = os.path.join(\"public_submission.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(submission_path, arcname=\"private_test.csv\")\n",
    "\n",
    "print(f\"Submission ZIP created: {zip_path}\")\n",
    "print(f\"File size: {os.path.getsize(zip_path) / 1024:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
